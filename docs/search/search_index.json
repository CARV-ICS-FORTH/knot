{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Overview Karvdash (Kubernetes CARV dashboard) is a service for facilitating data science in Kubernetes-based environments, by supplying the landing page for users, allowing them to launch notebooks and other services, design workflows, and specify parameters related to execution through a user-friendly interface. Karvdash aims to make it straightforward for domain experts to interact with resources in the underlying infrastructure without having to understand lower-level tools and mechanisms. In summary, Karvdash provides a web-based, graphical frontend - a dashboard - for users to: Launch services or applications from customizable templates. Manage files that are automatically attached to service and application containers when launched. Under the hood, Karvdash: Securely provisions multiple services under one externally-accessible HTTPS endpoint. Performs high-level user management and isolates respective services in per-user Kubernetes namespaces. Provides an identity service for authenticating users in OAuth 2.0/OIDC-compatible applications. Kubernetes provides all the tools to do data sharing, create namespaces, etc., but the exact implementation and structure of the overall environment is left to the developer. Karvdash implements a \"traditional\" user scheme, which is then mapped to Kubernetes namespaces and service accounts. Kubernetes does not have \"users\" and no place to \"login into\" by default. In addition, Karvdash wires up relevant storage to the appropriate paths inside running containers, which significantly simplifies data management. Private and shared files are automatically accessible in all execution contexts and also available through the dashboard.","title":"Overview"},{"location":"index.html#overview","text":"Karvdash (Kubernetes CARV dashboard) is a service for facilitating data science in Kubernetes-based environments, by supplying the landing page for users, allowing them to launch notebooks and other services, design workflows, and specify parameters related to execution through a user-friendly interface. Karvdash aims to make it straightforward for domain experts to interact with resources in the underlying infrastructure without having to understand lower-level tools and mechanisms. In summary, Karvdash provides a web-based, graphical frontend - a dashboard - for users to: Launch services or applications from customizable templates. Manage files that are automatically attached to service and application containers when launched. Under the hood, Karvdash: Securely provisions multiple services under one externally-accessible HTTPS endpoint. Performs high-level user management and isolates respective services in per-user Kubernetes namespaces. Provides an identity service for authenticating users in OAuth 2.0/OIDC-compatible applications. Kubernetes provides all the tools to do data sharing, create namespaces, etc., but the exact implementation and structure of the overall environment is left to the developer. Karvdash implements a \"traditional\" user scheme, which is then mapped to Kubernetes namespaces and service accounts. Kubernetes does not have \"users\" and no place to \"login into\" by default. In addition, Karvdash wires up relevant storage to the appropriate paths inside running containers, which significantly simplifies data management. Private and shared files are automatically accessible in all execution contexts and also available through the dashboard.","title":"Overview"},{"location":"install-bare-metal.html","text":"Installation on bare metal To install Karvdash on bare metal, first install Ubuntu Server 20.04 LTS on a server with an external IP address (tested on VirtualBox with 2 CPUs, 4 GB RAM, bridged network adapter). Update packages. Run as root. Set external IP address in an environment variable: export IP_ADDRESS=`ip -o route get 1 | sed -n 's/.*src \\([0-9.]\\+\\).*/\\1/p'` System Disable swap: sed -e '/swap/ s/^#*/#/' -i /etc/fstab swapoff -a Docker Follow these instructions to install Docker: apt-get update apt-get install -y ca-certificates curl gnupg lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install -y docker-ce docker-ce-cli containerd.io apt-mark hold docker-ce docker-ce-cli containerd.io Follow these instructions to configure Docker: mkdir -p /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF systemctl enable docker systemctl daemon-reload systemctl restart docker Kubernetes Follow these instructions to install kubeadm: KUBERNETES_VERSION=\"1.22.4\" apt-get update apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=${KUBERNETES_VERSION}-00 kubeadm=${KUBERNETES_VERSION}-00 kubectl=${KUBERNETES_VERSION}-00 apt-mark hold kubelet kubeadm kubectl Follow these instructions to initialize Kubernetes: kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=${KUBERNETES_VERSION} mkdir -p $HOME/.kube cp /etc/kubernetes/admin.conf $HOME/.kube/config kubectl taint nodes --all node-role.kubernetes.io/master- Install a Pod network add-on: FLANNEL_VERSION=\"0.15.1\" kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v${FLANNEL_VERSION}/Documentation/kube-flannel.yml Helm Download and install the Helm binary: HELM_VERSION=\"3.8.0\" curl -LO https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz tar -zxvf helm-v${HELM_VERSION}-linux-amd64.tar.gz cp linux-amd64/helm /usr/local/bin/ rm -rf helm-v${HELM_VERSION}-linux-amd64.tar.gz linux-amd64 helm plugin install https://github.com/databus23/helm-diff Helmfile Download and install the Helmfile binary: HELMFILE_VERSION=\"0.143.0\" curl -LO https://github.com/roboll/helmfile/releases/download/v${HELMFILE_VERSION}/helmfile_linux_amd64 cp helmfile_linux_amd64 /usr/local/bin/helmfile chmod +x /usr/local/bin/helmfile rm -f helmfile_linux_amd64 Karvdash Clone and deploy Karvdash : git clone https://github.com/kantale/OpenBio.eu.git git clone https://github.com/CARV-ICS-FORTH/karvdash.git cd karvdash apt-get install -y make BAREMETAL=yes make deploy-requirements BAREMETAL=yes make deploy-local","title":"Installation (bare metal)"},{"location":"install-bare-metal.html#installation-on-bare-metal","text":"To install Karvdash on bare metal, first install Ubuntu Server 20.04 LTS on a server with an external IP address (tested on VirtualBox with 2 CPUs, 4 GB RAM, bridged network adapter). Update packages. Run as root. Set external IP address in an environment variable: export IP_ADDRESS=`ip -o route get 1 | sed -n 's/.*src \\([0-9.]\\+\\).*/\\1/p'`","title":"Installation on bare metal"},{"location":"install-bare-metal.html#system","text":"Disable swap: sed -e '/swap/ s/^#*/#/' -i /etc/fstab swapoff -a","title":"System"},{"location":"install-bare-metal.html#docker","text":"Follow these instructions to install Docker: apt-get update apt-get install -y ca-certificates curl gnupg lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null apt-get update apt-get install -y docker-ce docker-ce-cli containerd.io apt-mark hold docker-ce docker-ce-cli containerd.io Follow these instructions to configure Docker: mkdir -p /etc/docker cat <<EOF | sudo tee /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF systemctl enable docker systemctl daemon-reload systemctl restart docker","title":"Docker"},{"location":"install-bare-metal.html#kubernetes","text":"Follow these instructions to install kubeadm: KUBERNETES_VERSION=\"1.22.4\" apt-get update apt-get install -y apt-transport-https ca-certificates curl curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet=${KUBERNETES_VERSION}-00 kubeadm=${KUBERNETES_VERSION}-00 kubectl=${KUBERNETES_VERSION}-00 apt-mark hold kubelet kubeadm kubectl Follow these instructions to initialize Kubernetes: kubeadm init --pod-network-cidr=10.244.0.0/16 --kubernetes-version=${KUBERNETES_VERSION} mkdir -p $HOME/.kube cp /etc/kubernetes/admin.conf $HOME/.kube/config kubectl taint nodes --all node-role.kubernetes.io/master- Install a Pod network add-on: FLANNEL_VERSION=\"0.15.1\" kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v${FLANNEL_VERSION}/Documentation/kube-flannel.yml","title":"Kubernetes"},{"location":"install-bare-metal.html#helm","text":"Download and install the Helm binary: HELM_VERSION=\"3.8.0\" curl -LO https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz tar -zxvf helm-v${HELM_VERSION}-linux-amd64.tar.gz cp linux-amd64/helm /usr/local/bin/ rm -rf helm-v${HELM_VERSION}-linux-amd64.tar.gz linux-amd64 helm plugin install https://github.com/databus23/helm-diff","title":"Helm"},{"location":"install-bare-metal.html#helmfile","text":"Download and install the Helmfile binary: HELMFILE_VERSION=\"0.143.0\" curl -LO https://github.com/roboll/helmfile/releases/download/v${HELMFILE_VERSION}/helmfile_linux_amd64 cp helmfile_linux_amd64 /usr/local/bin/helmfile chmod +x /usr/local/bin/helmfile rm -f helmfile_linux_amd64","title":"Helmfile"},{"location":"install-bare-metal.html#karvdash","text":"Clone and deploy Karvdash : git clone https://github.com/kantale/OpenBio.eu.git git clone https://github.com/CARV-ICS-FORTH/karvdash.git cd karvdash apt-get install -y make BAREMETAL=yes make deploy-requirements BAREMETAL=yes make deploy-local","title":"Karvdash"},{"location":"technical-notes.html","text":"Technical notes Storage management In a cluster environment, it is common for each user to have a \"home folder\", usually mounted over NFS, Lustre, Gluster, etc. Karvdash tries to apply this notion in a containerized environment: Given a cluster-wide shared folder, this folder is also mounted inside containers as well. Thus, when running a notebook server (like Jupyter), user data is available in the containerized environment at a well-known path - as it would be in a bare-metal cluster node. This, in addition to the web-based file browser provided by Karvdash, facilitates easy data management for applications, both for providing inputs and collecting outputs. In Karvdash, there are two such folders/data domains: Private: User data that is private to the user. Mounted in containers under /private . Shared: Data that is shared among all users. Mounted in containers under /shared . For the first domain Karvdash creates a subfolder for each user, named after the corresponding username and only allows access within that subfolder (like a \"home folder\"). This is hidden to the user, meaning that /private is the user subfolder itself. Users cannot go up a level and check other users' names and files. The dashboard runs as a service in Kubernetes and coordinates the execution of other services in particular namespaces. All provisioned containers of a user share common volumes. To attach these data folders to service and application containers, Karvdash creates Persistent Volumes and associated Persistent Volume Claims for each user, and provides a Kubernetes mutating admission webhook which intercepts all calls to create pods or deployments and injects the appropriate volumes to respective configurations before they are applied. The Karvdash service itself also has the same data folders mounted in order to present their contents via the dashboard. Also, another validating admission webhook makes sure that only allowed host paths can be mounted in pods. Remote datasets In addition to the \"private\" and \"shared\" data domains, Karvdash optionally interfaces with Datashim to mount internal or external S3 and H3 buckets to running containers. Karvdash provides the frontend to configure datasets and then attaches the produced Persistent Volume Claims to deployed pods. Datasets are mounted in containers at /mnt/datasets/<name> . Service templates Karvdash provides a way for users to easily configure and start services, by integrating a service templating mechanism based on Helm . Helm service templates, named \"charts\", are packaged and placed within an artifact registry, like Harbor . The list of available services includes global and user-specific charts, which are automatically discovered by Karvdash. When deploying a service, the user can specify chart values through the dashboard. Karvdash will silently set \"internal\" platform configuration values, such as the generated hostname assigned to the service, the location of the private container registry, etc. Karvdash-compatible charts may use the following values: Value Description Set in env. karvdash.enabled Set to true karvdash.hostname The hostname assigned to the service (set to <release name>-<username>.<ingress url> ) karvdash.username The user's username \u2713 karvdash.namespace The user's namespace \u2713 karvdash.ingressUrl The dashboard's URL \u2713 karvdash.privateDir The path to the \"private\" data domain \u2713 karvdash.privateVolume The volume used for the \"private\" data domain \u2713 karvdash.sharedDir The path to the \"shared\" data domain \u2713 karvdash.sharedVolume The volume used for the \"shared\" data domain \u2713 karvdash.argoWorkflowsUrl The URL of the Argo Worfklows service (set if Argo Workflows is enabled) \u2713 karvdash.privateRegistryUrl The URL of the \"private\" container registry (set if Harbor is enabled) \u2713 karvdash.publicRegistryUrl The URL of the \"shared\" container registry (set if Harbor is enabled) \u2713 karvdash.privateRepoUrl The URL of the \"private\" Helm chart repository (set if Harbor is enabled) \u2713 karvdash.publicRepoUrl The URL of the \"shared\" Helm chart repository (set if Harbor is enabled) \u2713 As shown in the table, some values are also set inside pods as environment variables (in uppercase snake case, i.e. KARVDASH_PRIVATE_DIR ). Karvdash will show all services to the user, except those marked with the label karvdash-hidden . Upon deployment, Karvdash will attach local storage folders to all pods, as well as remote datasets (except on pods labelled with karvdash-no-datasets ). Authentication directives are added to all ingress resources (except on those labelled with karvdash-no-auth ). User namespaces Internally, at the Kubernetes level, each Karvdash user is matched to a unique namespace, which also hosts all of the user's services. Containers launched within the namespace are given Kubernetes service accounts which are only allowed to operate within their own namespace. This practice organizes resources per user and isolates users from each other. For user \"test\", Karvdash creates the namespace karvdash-test and binds the default user account in that namespace to the cluster-admin cluster role (only for the karvdash-test namespace). Service exposure To expose services to the user, Karvdash makes use of a Kubernetes ingress - a proxy server. Service templates that provide a user-facing service include an ingress directive. Karvdash effectively: Exposes all services on subdomains of the main dashboard domain. These domains are composed of the service name and the username, so they can always be the same, allowing the user to bookmark the location. Protects all services with an authentication/authorization mechanism, by configuring each respective ingress to perform single sing-on through the dashboard. The default deployment integrates Vouch Proxy as an OAuth 2.0/OIDC client to the dashboard, which in turn provides credentials to the NGINX-based web proxy implementing the ingress. Thus, each service can only be accessed by its owner. This helps avoiding any external party visiting a user's service frontend without appropriate credentials. Incorporates all services under a common SSL environment, so all data sent back-and-forth through each ingress is encrypted. Assuming that the dashboard is accessible at example.com , the \"File Browser\" service named browser started by user \"test\" will be exposed at browser-test.example.com . Karvdash will also inject appropriate rules to the service's ingress configuration, so that no other user can access browser-test.example.com . As the ingress will be configured with an SSL certificate for both example.com and *.example.com , all connections will be SSL terminated. SSO service Karvdash implements an OAuth 2.0/OIDC provider, which allows third-party services to request verification of users' identities via standard protocols. In the OIDC response, Karvdash also sets extra data that may be useful to connected services (all environment variables mentioned in Service templates , but in lowercase, i.e. karvdash_private_dir ). Note that OAuth 2.0/OIDC provides only authentication information and it is up to the connecting service to define what users are authorized to do, based on their identities (i.e., username, email, etc.). In addition to the integration with Vouch Proxy for authenticating users to services started by the dashboard, Karvdash also acts as an identity provider to JupyterHub , Argo Workflows , Harbor , and other services that may be installed side-by-side to the dashboard. For compatible services, Karvdash also configures user authorization to resources. For example, in Argo Workflows, Karvdash sets the appropriate role bindings so that users will only be allowed to access workflows in their respective Karvdash-defined namespaces.","title":"Technical notes"},{"location":"technical-notes.html#technical-notes","text":"","title":"Technical notes"},{"location":"technical-notes.html#storage-management","text":"In a cluster environment, it is common for each user to have a \"home folder\", usually mounted over NFS, Lustre, Gluster, etc. Karvdash tries to apply this notion in a containerized environment: Given a cluster-wide shared folder, this folder is also mounted inside containers as well. Thus, when running a notebook server (like Jupyter), user data is available in the containerized environment at a well-known path - as it would be in a bare-metal cluster node. This, in addition to the web-based file browser provided by Karvdash, facilitates easy data management for applications, both for providing inputs and collecting outputs. In Karvdash, there are two such folders/data domains: Private: User data that is private to the user. Mounted in containers under /private . Shared: Data that is shared among all users. Mounted in containers under /shared . For the first domain Karvdash creates a subfolder for each user, named after the corresponding username and only allows access within that subfolder (like a \"home folder\"). This is hidden to the user, meaning that /private is the user subfolder itself. Users cannot go up a level and check other users' names and files. The dashboard runs as a service in Kubernetes and coordinates the execution of other services in particular namespaces. All provisioned containers of a user share common volumes. To attach these data folders to service and application containers, Karvdash creates Persistent Volumes and associated Persistent Volume Claims for each user, and provides a Kubernetes mutating admission webhook which intercepts all calls to create pods or deployments and injects the appropriate volumes to respective configurations before they are applied. The Karvdash service itself also has the same data folders mounted in order to present their contents via the dashboard. Also, another validating admission webhook makes sure that only allowed host paths can be mounted in pods.","title":"Storage management"},{"location":"technical-notes.html#remote-datasets","text":"In addition to the \"private\" and \"shared\" data domains, Karvdash optionally interfaces with Datashim to mount internal or external S3 and H3 buckets to running containers. Karvdash provides the frontend to configure datasets and then attaches the produced Persistent Volume Claims to deployed pods. Datasets are mounted in containers at /mnt/datasets/<name> .","title":"Remote datasets"},{"location":"technical-notes.html#service-templates","text":"Karvdash provides a way for users to easily configure and start services, by integrating a service templating mechanism based on Helm . Helm service templates, named \"charts\", are packaged and placed within an artifact registry, like Harbor . The list of available services includes global and user-specific charts, which are automatically discovered by Karvdash. When deploying a service, the user can specify chart values through the dashboard. Karvdash will silently set \"internal\" platform configuration values, such as the generated hostname assigned to the service, the location of the private container registry, etc. Karvdash-compatible charts may use the following values: Value Description Set in env. karvdash.enabled Set to true karvdash.hostname The hostname assigned to the service (set to <release name>-<username>.<ingress url> ) karvdash.username The user's username \u2713 karvdash.namespace The user's namespace \u2713 karvdash.ingressUrl The dashboard's URL \u2713 karvdash.privateDir The path to the \"private\" data domain \u2713 karvdash.privateVolume The volume used for the \"private\" data domain \u2713 karvdash.sharedDir The path to the \"shared\" data domain \u2713 karvdash.sharedVolume The volume used for the \"shared\" data domain \u2713 karvdash.argoWorkflowsUrl The URL of the Argo Worfklows service (set if Argo Workflows is enabled) \u2713 karvdash.privateRegistryUrl The URL of the \"private\" container registry (set if Harbor is enabled) \u2713 karvdash.publicRegistryUrl The URL of the \"shared\" container registry (set if Harbor is enabled) \u2713 karvdash.privateRepoUrl The URL of the \"private\" Helm chart repository (set if Harbor is enabled) \u2713 karvdash.publicRepoUrl The URL of the \"shared\" Helm chart repository (set if Harbor is enabled) \u2713 As shown in the table, some values are also set inside pods as environment variables (in uppercase snake case, i.e. KARVDASH_PRIVATE_DIR ). Karvdash will show all services to the user, except those marked with the label karvdash-hidden . Upon deployment, Karvdash will attach local storage folders to all pods, as well as remote datasets (except on pods labelled with karvdash-no-datasets ). Authentication directives are added to all ingress resources (except on those labelled with karvdash-no-auth ).","title":"Service templates"},{"location":"technical-notes.html#user-namespaces","text":"Internally, at the Kubernetes level, each Karvdash user is matched to a unique namespace, which also hosts all of the user's services. Containers launched within the namespace are given Kubernetes service accounts which are only allowed to operate within their own namespace. This practice organizes resources per user and isolates users from each other. For user \"test\", Karvdash creates the namespace karvdash-test and binds the default user account in that namespace to the cluster-admin cluster role (only for the karvdash-test namespace).","title":"User namespaces"},{"location":"technical-notes.html#service-exposure","text":"To expose services to the user, Karvdash makes use of a Kubernetes ingress - a proxy server. Service templates that provide a user-facing service include an ingress directive. Karvdash effectively: Exposes all services on subdomains of the main dashboard domain. These domains are composed of the service name and the username, so they can always be the same, allowing the user to bookmark the location. Protects all services with an authentication/authorization mechanism, by configuring each respective ingress to perform single sing-on through the dashboard. The default deployment integrates Vouch Proxy as an OAuth 2.0/OIDC client to the dashboard, which in turn provides credentials to the NGINX-based web proxy implementing the ingress. Thus, each service can only be accessed by its owner. This helps avoiding any external party visiting a user's service frontend without appropriate credentials. Incorporates all services under a common SSL environment, so all data sent back-and-forth through each ingress is encrypted. Assuming that the dashboard is accessible at example.com , the \"File Browser\" service named browser started by user \"test\" will be exposed at browser-test.example.com . Karvdash will also inject appropriate rules to the service's ingress configuration, so that no other user can access browser-test.example.com . As the ingress will be configured with an SSL certificate for both example.com and *.example.com , all connections will be SSL terminated.","title":"Service exposure"},{"location":"technical-notes.html#sso-service","text":"Karvdash implements an OAuth 2.0/OIDC provider, which allows third-party services to request verification of users' identities via standard protocols. In the OIDC response, Karvdash also sets extra data that may be useful to connected services (all environment variables mentioned in Service templates , but in lowercase, i.e. karvdash_private_dir ). Note that OAuth 2.0/OIDC provides only authentication information and it is up to the connecting service to define what users are authorized to do, based on their identities (i.e., username, email, etc.). In addition to the integration with Vouch Proxy for authenticating users to services started by the dashboard, Karvdash also acts as an identity provider to JupyterHub , Argo Workflows , Harbor , and other services that may be installed side-by-side to the dashboard. For compatible services, Karvdash also configures user authorization to resources. For example, in Argo Workflows, Karvdash sets the appropriate role bindings so that users will only be allowed to access workflows in their respective Karvdash-defined namespaces.","title":"SSO service"},{"location":"user-guide.html","text":"User guide This guide walks you through the various Karvdash screens, starting from user sign up, explaining the available functions. Sign up and login When you visit the dashboard service with your browser, you are greeted with the login screen. To create an account, select the \"Sign up\" option on the main screen and fill in a username, password, and contact email. Once the account is activated by an administrator, login using your username and password. You can change your password when logged in by clicking on the user icon at the top-right of the screen and selecting \"Change password\" from the menu. The menu also provides options to report an issue, access this documentation, and logout. If you ever forget your password, please ask an administrator to reset it. Services screen The services screen is accessed by selecting \"Services\" from the menu on the left. You are presented with a list of running services. Select a service name and you will be taken to the service frontend in a new browser tab. Select the \"Actions\" button to remove a service. \ud83d\udcdd The service may take some time to initialize. If you select a service name immediately after creation, you may see a proxy error. Just wait for a few seconds and refresh your browser. To start a new service, click on the respective button on the right. You will be shown a list of available service templates. Choose one and click \"Create\". The next screen is where you can define service variables. You can optionally change the service name to one that is easier to remember (if a name is already taken, Karvdash will append random characters). Besides the name, each service template has different variables. When done, click \"Create\" again, and you will be taken back to the service list, which should contain your new service (a message on the top of the screen will verify that a new service started and provide its name). Templates screen The templates screen is accessed by selecting \"Templates\" from the menu on the left. You are presented with a list of available service templates. Select a template to download it in YAML format. Select the \"Actions\" button to delete a template (only user templates can be deleted) or start a service from it. To add a new template, click on the respective button on the right. The template structure is described in the Service templates chapter. Datasets screen \ud83d\udcdd Dataset management is an optional Karvdash feature that may have not been enabled in your deployment. The datasets screen is accessed by selecting \"Datasets\" from the menu on the left. You are presented with a list of configured datasets. Select a dataset to download its configuration in YAML format. Select the \"Actions\" button to delete a dataset. Datasets are mounted in containers under /mnt/datasets/<name> . To add a new dataset, click on the respective button on the right. You will be shown a list of available dataset types. Choose one and click \"Add\". The next screen is where you can define the dataset configuration. You can optionally change the dataset name to one that is easier to remember (if a name is already taken, Karvdash will append random characters). Besides the name, each dataset type has different configuration options. When done, click \"Add\" again, and you will be taken back to the datasets list, which should contain your new dataset (a message on the top of the screen will verify that a new dataset has been added and provide its name). Files screen The files screen is accessed by selecting \"Files\" from the menu on the left. You are presented with a list of folders and files in the respective domain. Change domain (\"private\" or \"shared\") by clicking on the corresponding buttons on the upper-right of the screen. The \"private\" domain contains private user files, while \"shared\" is common across all users. Any user can add or remove files in \"shared\". Select a folder to navigate into that path (the current path is shown above the list), or a file to download it. Select the \"Actions\" button to download a folder as an archive or delete an object. Files are mounted in containers under /private and /shared respectively. To add a new folder or upload file(s) at the current path, click on the respective buttons on the right. Note that you can not overwrite an existing folder or file. \ud83d\udcdd The \"Files\" screen is meant to provide the very basic of file-related operations. Use the notebook environment as you would use a shell on a UNIX-based machine to control the filesystem in a more elaborate manner, or create a \"File Browser\" service for a web-based management interface on a specific folder. Administration \ud83d\udcdd The information in this section applies only to administrators. The \"admin\" user has access to an additional screen named \"Users\". The users screen is accessed by selecting \"Users\" from the menu on the left. You are presented with a list of users, by username. Each user can be \"active\", meaning with access to the dashboard and services. Each user can also be promoted to an administrator. The respective actions are available in the menu presented when selecting the \"Actions\" button. An administrator can edit a user's email, change a user's password, impersonate, and delete a user. When impersonating another user, the whole interface changes to what the user sees and the user icon at the top-right of the screen darkens to signify \"impersonation mode\". The user menu provides the option to stop impersonating and return to the original user's view.","title":"User guide"},{"location":"user-guide.html#user-guide","text":"This guide walks you through the various Karvdash screens, starting from user sign up, explaining the available functions.","title":"User guide"},{"location":"user-guide.html#sign-up-and-login","text":"When you visit the dashboard service with your browser, you are greeted with the login screen. To create an account, select the \"Sign up\" option on the main screen and fill in a username, password, and contact email. Once the account is activated by an administrator, login using your username and password. You can change your password when logged in by clicking on the user icon at the top-right of the screen and selecting \"Change password\" from the menu. The menu also provides options to report an issue, access this documentation, and logout. If you ever forget your password, please ask an administrator to reset it.","title":"Sign up and login"},{"location":"user-guide.html#services-screen","text":"The services screen is accessed by selecting \"Services\" from the menu on the left. You are presented with a list of running services. Select a service name and you will be taken to the service frontend in a new browser tab. Select the \"Actions\" button to remove a service. \ud83d\udcdd The service may take some time to initialize. If you select a service name immediately after creation, you may see a proxy error. Just wait for a few seconds and refresh your browser. To start a new service, click on the respective button on the right. You will be shown a list of available service templates. Choose one and click \"Create\". The next screen is where you can define service variables. You can optionally change the service name to one that is easier to remember (if a name is already taken, Karvdash will append random characters). Besides the name, each service template has different variables. When done, click \"Create\" again, and you will be taken back to the service list, which should contain your new service (a message on the top of the screen will verify that a new service started and provide its name).","title":"Services screen"},{"location":"user-guide.html#templates-screen","text":"The templates screen is accessed by selecting \"Templates\" from the menu on the left. You are presented with a list of available service templates. Select a template to download it in YAML format. Select the \"Actions\" button to delete a template (only user templates can be deleted) or start a service from it. To add a new template, click on the respective button on the right. The template structure is described in the Service templates chapter.","title":"Templates screen"},{"location":"user-guide.html#datasets-screen","text":"\ud83d\udcdd Dataset management is an optional Karvdash feature that may have not been enabled in your deployment. The datasets screen is accessed by selecting \"Datasets\" from the menu on the left. You are presented with a list of configured datasets. Select a dataset to download its configuration in YAML format. Select the \"Actions\" button to delete a dataset. Datasets are mounted in containers under /mnt/datasets/<name> . To add a new dataset, click on the respective button on the right. You will be shown a list of available dataset types. Choose one and click \"Add\". The next screen is where you can define the dataset configuration. You can optionally change the dataset name to one that is easier to remember (if a name is already taken, Karvdash will append random characters). Besides the name, each dataset type has different configuration options. When done, click \"Add\" again, and you will be taken back to the datasets list, which should contain your new dataset (a message on the top of the screen will verify that a new dataset has been added and provide its name).","title":"Datasets screen"},{"location":"user-guide.html#files-screen","text":"The files screen is accessed by selecting \"Files\" from the menu on the left. You are presented with a list of folders and files in the respective domain. Change domain (\"private\" or \"shared\") by clicking on the corresponding buttons on the upper-right of the screen. The \"private\" domain contains private user files, while \"shared\" is common across all users. Any user can add or remove files in \"shared\". Select a folder to navigate into that path (the current path is shown above the list), or a file to download it. Select the \"Actions\" button to download a folder as an archive or delete an object. Files are mounted in containers under /private and /shared respectively. To add a new folder or upload file(s) at the current path, click on the respective buttons on the right. Note that you can not overwrite an existing folder or file. \ud83d\udcdd The \"Files\" screen is meant to provide the very basic of file-related operations. Use the notebook environment as you would use a shell on a UNIX-based machine to control the filesystem in a more elaborate manner, or create a \"File Browser\" service for a web-based management interface on a specific folder.","title":"Files screen"},{"location":"user-guide.html#administration","text":"\ud83d\udcdd The information in this section applies only to administrators. The \"admin\" user has access to an additional screen named \"Users\". The users screen is accessed by selecting \"Users\" from the menu on the left. You are presented with a list of users, by username. Each user can be \"active\", meaning with access to the dashboard and services. Each user can also be promoted to an administrator. The respective actions are available in the menu presented when selecting the \"Actions\" button. An administrator can edit a user's email, change a user's password, impersonate, and delete a user. When impersonating another user, the whole interface changes to what the user sees and the user icon at the top-right of the screen darkens to signify \"impersonation mode\". The user menu provides the option to stop impersonating and return to the original user's view.","title":"Administration"}]}